{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252c493d-d50b-4505-94f6-2cbe716e3527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd8cf50-27fc-47eb-8295-4b34e4690e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "import logging\n",
    "from functools import cached_property\n",
    "from time import time\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from fastcore.basics import class2attr\n",
    "from scipy.spatial.transform import Rotation\n",
    "from tmenv.gbx import ghost_wr_from_map_uid\n",
    "from tmenv.trajectory import Trajectory\n",
    "from torchvision.transforms import Resize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb1849ab-58fd-4d17-be32-a36490866a7d",
   "metadata": {},
   "source": [
    "# Wrappers\n",
    "> Env wrappers for tmenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1983b2a-39cc-4140-875c-5d739161ebbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class StateWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, state_space):\n",
    "        super().__init__(env)\n",
    "        self.observation_space[self.observation_name] = state_space\n",
    "\n",
    "    @property\n",
    "    def observation_name(self):\n",
    "        return class2attr(self, \"(StateWrapper|Wrapper|State)\")\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, done, info = self.env.step(action)\n",
    "        observation[self.observation_name] = self.state(observation)\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def respawn(self, launched=True):\n",
    "        observation = self.env.respawn(launched)\n",
    "        observation[self.observation_name] = self.state(observation)\n",
    "        return observation\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        observation = self.env.reset(**kwargs)\n",
    "        observation[self.observation_name] = self.state(observation)\n",
    "        return observation\n",
    "\n",
    "    def state(self, observation):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88bc3bb-adb7-45ff-bf30-0bb7a12d1f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class VisualState(StateWrapper):\n",
    "    def __init__(self, env, shape, div=255.0):\n",
    "        if isinstance(shape, int):\n",
    "            shape = (shape, shape)\n",
    "        self.shape, self.div = shape, div\n",
    "        self.resize = Resize(shape, antialias=True)\n",
    "        super().__init__(env, gym.spaces.Box(0, 1, (3, *shape)))\n",
    "\n",
    "    def state(self, observation):\n",
    "        return (\n",
    "            self.resize(self.game.capture_tensor().to(self.device, non_blocking=True))\n",
    "            .float()\n",
    "            .div(self.div)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2db2ca-c6cd-4193-9fe6-9636dd4a4dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def vec2np(vec):\n",
    "    return np.array(vec, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d58d8f8-3219-4d39-af50-6976c0156f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "XYZ = np.array([[0, 0, 1], [1, 0, 0], [0, 1, 0]])\n",
    "\n",
    "\n",
    "def vec2rotation(forward, left, up):\n",
    "    r, d = Rotation.align_vectors(XYZ, [forward, left, up])\n",
    "    assert d < 1e-2, f\"rmsd is {d} wich is greater that 1e-2\"\n",
    "    return r\n",
    "\n",
    "\n",
    "def angle_diff(a, b):\n",
    "    return np.arctan2(np.sin(b - a), np.cos(b - a))\n",
    "\n",
    "\n",
    "class RotationEnvWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.rotation = None\n",
    "\n",
    "    def _rotation(self, env_obs):\n",
    "        previous_rotation = vec2rotation(\n",
    "            env_obs[\"previous\"][\"forward\"],\n",
    "            env_obs[\"previous\"][\"left\"],\n",
    "            env_obs[\"previous\"][\"up\"],\n",
    "        )\n",
    "        previous_yaw, previous_pitch, previous_roll = previous_rotation.as_euler(\"yxz\")\n",
    "        self.rotation = vec2rotation(\n",
    "            env_obs[\"forward\"],\n",
    "            env_obs[\"left\"],\n",
    "            env_obs[\"up\"],\n",
    "        )\n",
    "        yaw, pitch, roll = self.rotation.as_euler(\"yxz\")\n",
    "        time_diff = env_obs[\"_updatetime\"] - env_obs[\"previous\"][\"_updatetime\"]\n",
    "        if time_diff < 1e-4:\n",
    "            yaw_speed = 0\n",
    "            pitch_speed = 0\n",
    "            roll_speed = 0\n",
    "        else:\n",
    "            yaw_speed = angle_diff(previous_yaw, yaw) / time_diff\n",
    "            pitch_speed = angle_diff(previous_pitch, pitch) / time_diff\n",
    "            roll_speed = angle_diff(previous_roll, roll) / time_diff\n",
    "        velocity = self.rotation.apply(env_obs[\"velocity\"])\n",
    "        velocity_norm = np.linalg.norm(velocity)\n",
    "        velocity_normed = velocity / velocity_norm if velocity_norm > 1e-3 else velocity\n",
    "\n",
    "        return dict(\n",
    "            yaw=yaw.item(),\n",
    "            pitch=pitch.item(),\n",
    "            roll=roll.item(),\n",
    "            yaw_speed=yaw_speed.item(),\n",
    "            pitch_speed=pitch_speed.item(),\n",
    "            roll_speed=roll_speed.item(),\n",
    "            velocity=velocity.tolist(),\n",
    "            velocity_norm=velocity_norm.item(),\n",
    "            velocity_normed=velocity_normed.tolist(),\n",
    "        )\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, done, info = self.env.step(action)\n",
    "        env = observation[\"env\"]\n",
    "        env[\"rotation\"] = self._rotation(env)\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def respawn(self, launched=True):\n",
    "        observation = self.env.respawn(launched)\n",
    "        env = observation[\"env\"]\n",
    "        env[\"rotation\"] = self._rotation(env)\n",
    "        return observation\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        observation = self.env.reset(**kwargs)\n",
    "        env = observation[\"env\"]\n",
    "        env[\"rotation\"] = self._rotation(env)\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b6b126-d919-4fda-adf4-63cc13f98deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class TrajectoryEnvWrapper(gym.Wrapper):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        max_distance,\n",
    "        ghost_types=[\"author\", \"wr\"],\n",
    "        jump_z_coef=0.5,  # Distance to trajectory on z axis coef when the reference trajectory is jumping\n",
    "        minimum_speed=5,\n",
    "        maximum_travel=300,\n",
    "    ):\n",
    "        super().__init__(env)\n",
    "        self.max_distance = max_distance\n",
    "        self.ghost_types = ghost_types\n",
    "        self.jump_z_coef = jump_z_coef\n",
    "        self.minimum_speed = minimum_speed\n",
    "        self.maximum_travel = maximum_travel\n",
    "        self.ghost_data = None\n",
    "        self.trajectory = None\n",
    "\n",
    "    def load_map(self, map_data):\n",
    "        self.env.load_map(map_data)\n",
    "        for ghost_type in self.ghost_types:\n",
    "            if ghost_type in self.map_data[\"ghosts\"]:\n",
    "                self.ghost_data = self.map_data[\"ghosts\"][ghost_type]\n",
    "                logging.info(\n",
    "                    f\"Using {ghost_type} ghost for map {self.map_data['name']}\"\n",
    "                )\n",
    "                break\n",
    "        assert (\n",
    "            self.ghost_data is not None\n",
    "        ), f\"No ghost found for map {self.map_data['name']}\"\n",
    "        self.trajectory = Trajectory(self.ghost_data)\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, done, info = self.env.step(action)\n",
    "        env = observation[\"env\"]\n",
    "        traj_obs = self.trajectory.observation(\n",
    "            vec2np(env[\"position\"]), len(self.checkpoints), self.standing_cp_offset, self.jump_z_coef\n",
    "        )\n",
    "        traj_obs[\"progress\"] = (\n",
    "            1.0 if env[\"finished\"] else min(0.999, traj_obs[\"progress\"])\n",
    "        )\n",
    "\n",
    "        traj_obs[\"too_far\"] = self.max_distance < traj_obs[\"weighted_distance\"]\n",
    "        traj_obs[\"furthest\"] = self.progress < traj_obs[\"progress\"]\n",
    "        # prevent ultra low speed to count as progress (turtle or facing wall)\n",
    "        traj_obs[\"furthest\"] &= self.minimum_speed < (env[\"velocity_norm\"] * 3.6)\n",
    "        # prevent big cut to count as progress (these are not true cut, AI is stuck afterwards or take wrong cp)\n",
    "        traj_obs[\"furthest\"] &= traj_obs[\"travel\"] - self.travel < self.maximum_travel\n",
    "        traj_obs[\"mistake\"] = traj_obs[\"too_far\"] or not traj_obs[\"furthest\"]\n",
    "\n",
    "        if traj_obs[\"mistake\"]:\n",
    "            traj_obs[\"mistake_duration\"] = time() - self.mistake_start_time\n",
    "        else:\n",
    "            self.mistake_start_time = time()\n",
    "            traj_obs[\"mistake_duration\"] = 0\n",
    "\n",
    "        if traj_obs[\"furthest\"]:\n",
    "            traj_obs[\"step_travel\"] = traj_obs[\"travel\"] - self.travel\n",
    "            self.travel = traj_obs[\"travel\"]\n",
    "            self.progress = traj_obs[\"progress\"]\n",
    "            if self.max_progress < self.progress:\n",
    "                self.max_progress = self.progress\n",
    "        else:\n",
    "            traj_obs[\"step_travel\"] = 0\n",
    "            traj_obs[\"progress\"] = self.progress\n",
    "\n",
    "        env[\"trajectory\"] = traj_obs\n",
    "        info[\"progress\"] = traj_obs[\"progress\"]\n",
    "        info[\"max_progress\"] = self.max_progress\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def respawn(self, launched=True):\n",
    "        observation = self.env.respawn(launched)\n",
    "        env = observation[\"env\"]\n",
    "        traj_obs = self.trajectory.observation(\n",
    "            vec2np(env[\"position\"]), len(self.checkpoints), self.standing_cp_offset, self.jump_z_coef\n",
    "        )\n",
    "\n",
    "        self.progress = traj_obs[\"progress\"]\n",
    "        self.mistake_start_time = time()\n",
    "        self.travel = traj_obs[\"travel\"]\n",
    "        traj_obs[\"missed_checkpoint\"] = False\n",
    "        traj_obs[\"too_far\"] = False\n",
    "        traj_obs[\"furthest\"] = True\n",
    "        traj_obs[\"step_travel\"] = 0\n",
    "        traj_obs[\"mistake_duration\"] = 0\n",
    "\n",
    "        env[\"trajectory\"] = traj_obs\n",
    "        return observation\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        observation = self.env.reset(**kwargs)\n",
    "        env = observation[\"env\"]\n",
    "        traj_obs = self.trajectory.observation(\n",
    "            vec2np(env[\"position\"]), len(self.checkpoints), self.standing_cp_offset, self.jump_z_coef\n",
    "        )\n",
    "\n",
    "        self.progress = 0\n",
    "        self.max_progress = 0\n",
    "        self.mistake_start_time = time()\n",
    "        self.travel = 0\n",
    "        traj_obs[\"missed_checkpoint\"] = False\n",
    "        traj_obs[\"too_far\"] = False\n",
    "        traj_obs[\"furthest\"] = True\n",
    "        traj_obs[\"step_travel\"] = 0\n",
    "        traj_obs[\"mistake_duration\"] = 0\n",
    "        traj_obs[\"progress\"] = 0\n",
    "\n",
    "        env[\"trajectory\"] = traj_obs\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8f4868-8c96-4057-b370-fa42702f982f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class TelemetryState(StateWrapper):\n",
    "    def __init__(self, env, telemetry_lag_divisor=20, env_lag_divisor=0.1):\n",
    "        self.shape = (66,)\n",
    "        super().__init__(env, gym.spaces.Box(-1.0, 1.0, self.shape))\n",
    "        self.telemetry_lag_divisor = telemetry_lag_divisor\n",
    "        self.env_lag_divisor = env_lag_divisor\n",
    "        assert hasattr(self, \"rotation\"), \"Env must have RotationEnvWrapper\"\n",
    "        assert hasattr(self, \"trajectory\"), \"Env must have TrajectoryEnvWrapper\"\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, done, info = self.env.step(action)\n",
    "        observation[self.observation_name] = self.state(\n",
    "            observation, self.action_handler.action_dict(action)\n",
    "        )\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def state(self, observation, action=dict(accelerate=False, brake=False, steer=0.0)):\n",
    "        env = observation[\"env\"]\n",
    "        return torch.tensor(\n",
    "            [\n",
    "                action[\"accelerate\"],\n",
    "                action[\"brake\"],\n",
    "                action[\"steer\"],\n",
    "                env[\"telemetry_lag\"] / self.telemetry_lag_divisor,\n",
    "                (time() - env[\"observation_start_time\"]) / self.env_lag_divisor,\n",
    "                env[\"finished\"],\n",
    "                env[\"accel_coef\"],\n",
    "                env[\"control_coef\"],\n",
    "                env[\"gravity_coef\"],\n",
    "                env[\"adherence_coef\"],\n",
    "                env[\"upwardness\"],\n",
    "                env[\"wheels_contact_count\"] / 4.0,\n",
    "                env[\"wheels_skidding_count\"] / 4.0,\n",
    "                env[\"flying_duration\"] > 0,\n",
    "                env[\"skidding_duration\"] > 0,\n",
    "                env[\"handicap_no_gas_duration\"] > 0,\n",
    "                env[\"handicap_force_gas_duration\"] > 0,\n",
    "                env[\"handicap_no_brakes_duration\"] > 0,\n",
    "                env[\"handicap_no_steering_duration\"] > 0,\n",
    "                env[\"handicap_no_grip_duration\"] > 0,\n",
    "                *[env[\"reactor_boost_level\"] == i for i in range(3)],\n",
    "                *[env[\"reactor_boost_type\"] == i for i in range(4)],\n",
    "                env[\"reactor_inputs_x\"],\n",
    "                env[\"is_ground_contact\"],\n",
    "                env[\"is_reactor_ground_mode\"],\n",
    "                env[\"is_wheels_burning\"],\n",
    "                env[\"ground_dist\"] / 10,\n",
    "                *[env[\"gear\"] == i for i in range(1, 6)],\n",
    "                env[\"engine_on\"],\n",
    "                env[\"is_turbo\"],\n",
    "                env[\"turbo_time\"],\n",
    "                env[\"bullet_time_normed\"],\n",
    "                env[\"simulation_time_coef\"],\n",
    "                env[\"air_brake_normed\"],\n",
    "                env[\"spoiler_open_normed\"],\n",
    "                env[\"wings_open_normed\"],\n",
    "                env[\"is_top_contact\"],\n",
    "                env[\"wetness\"],\n",
    "                env[\"water_immersion_coef\"],\n",
    "                env[\"water_over_dist_normed\"],\n",
    "                env[\"front_speed\"] * 3.6 / 500,\n",
    "                env[\"front_speed\"] < 0,\n",
    "                env[\"velocity_norm\"] * 3.6 / 500,\n",
    "                env[\"rpm\"] / 11000.0,\n",
    "                env[\"side_speed\"] * 3.6 / 500,\n",
    "                env[\"rotation\"][\"pitch\"],\n",
    "                env[\"rotation\"][\"roll\"],\n",
    "                env[\"rotation\"][\"yaw_speed\"],\n",
    "                env[\"rotation\"][\"pitch_speed\"],\n",
    "                env[\"rotation\"][\"roll_speed\"],\n",
    "                env[\"rotation\"][\"velocity_norm\"],\n",
    "                *env[\"rotation\"][\"velocity_normed\"],\n",
    "                env[\"trajectory\"][\"missed_checkpoint\"],\n",
    "                env[\"trajectory\"][\"furthest\"],\n",
    "                env[\"trajectory\"][\"too_far\"],\n",
    "            ],\n",
    "            dtype=torch.float,\n",
    "        ).to(self.device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd280023-cf62-4130-a8c6-c8fe598aa33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class WheelState(StateWrapper):\n",
    "    num_contact_classes = 82\n",
    "    wheel_types = [\"fl_wheel\", \"fr_wheel\", \"rl_wheel\", \"rr_wheel\"]\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self.shape = (self.num_contact_classes + 8, 4)\n",
    "        super().__init__(env, gym.spaces.Box(-1.0, 1.0, self.shape))\n",
    "\n",
    "    def wheel_state(self, observation, wheel_type):\n",
    "        env = observation[\"env\"]\n",
    "        wheel_state = torch.tensor(\n",
    "            [\n",
    "                env[wheel_type + \"_steer_angle\"],\n",
    "                env[wheel_type + \"_rotation\"] / 10000,\n",
    "                env[wheel_type + \"_damper_len\"],\n",
    "                env[wheel_type + \"_slip_coef\"],\n",
    "                env[wheel_type + \"_dirt\"],\n",
    "                env[wheel_type + \"_break_coef\"],\n",
    "                env[wheel_type + \"_tire_wear\"],\n",
    "                env[wheel_type + \"_icing\"],\n",
    "            ],\n",
    "            dtype=torch.float,\n",
    "        ).to(self.device, non_blocking=True)\n",
    "        contact_state = F.one_hot(\n",
    "            torch.tensor(env[wheel_type + \"_contact_material\"]).to(\n",
    "                self.device, non_blocking=True\n",
    "            ),\n",
    "            num_classes=self.num_contact_classes,\n",
    "        ).squeeze()\n",
    "        return torch.cat([wheel_state, contact_state])\n",
    "\n",
    "    def state(self, observation):\n",
    "        states = [self.wheel_state(observation, t) for t in self.wheel_types]\n",
    "        return torch.stack(\n",
    "            states,\n",
    "            dim=1,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2193a0-0389-4c8d-8cc5-e8bca0b3483c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class TrajectoryState(StateWrapper):\n",
    "    def __init__(self, env, nb_positions, spacing):\n",
    "        self.shape = (4, nb_positions)\n",
    "        super().__init__(env, gym.spaces.Box(-1.0, 1.0, self.shape))\n",
    "        self.nb_positions = nb_positions\n",
    "        self.spacing = spacing\n",
    "        assert hasattr(self, \"trajectory\"), \"Env must have TrajectoryEnvWrapper\"\n",
    "        assert hasattr(self, \"rotation\"), \"Env must have RotationEnvWrapper\"\n",
    "\n",
    "    def state(self, observation):\n",
    "        return self.trajectory.frame_trajectory(\n",
    "            vec2np(observation[\"env\"][\"position\"]),\n",
    "            self.rotation,\n",
    "            len(self.checkpoints),\n",
    "            self.standing_cp_offset,\n",
    "            self.nb_positions,\n",
    "            self.spacing,\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "class AlignedState(StateWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env, gym.spaces.Box(-1.0, 1.0, (1,)))\n",
    "        assert hasattr(self, \"trajectory\"), \"Env must have TrajectoryEnvWrapper\"\n",
    "\n",
    "    def state(self, observation):\n",
    "        aligned = self.trajectory.aligned(\n",
    "            vec2np(observation[\"env\"][\"position\"]),\n",
    "            len(self.checkpoints),\n",
    "            self.standing_cp_offset,\n",
    "            vec2np(observation[\"env\"][\"velocity\"]),\n",
    "            device=self.device,\n",
    "        )\n",
    "        observation[\"env\"][\"aligned\"] = bool(aligned)\n",
    "        return aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176e8abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class PositionalEncoder:\n",
    "    def __init__(self, d_model, min_freq=1e-4, device=\"cpu\"):\n",
    "        self.d_model = d_model\n",
    "        self.min_freq = min_freq\n",
    "        self.device = device\n",
    "        self.freqs = min_freq ** (\n",
    "            2 * torch.arange(d_model).div(2, rounding_mode=\"floor\") / d_model\n",
    "        ).reshape(1, -1).to(self.device)\n",
    "\n",
    "    def __call__(self, position):\n",
    "        pos_enc = position.reshape(-1, 1) * self.freqs\n",
    "        pos_enc[:, ::2] = torch.cos(pos_enc[:, ::2])\n",
    "        pos_enc[:, 1::2] = torch.sin(pos_enc[:, 1::2])\n",
    "        return pos_enc\n",
    "\n",
    "class MapState(StateWrapper):\n",
    "    def __init__(self, env, d_model=16):\n",
    "        self.shape = (3 * d_model,)\n",
    "        super().__init__(env, gym.spaces.Box(-1.0, 1.0, self.shape))\n",
    "        self.penc = PositionalEncoder(d_model, 1e-5, self.device)\n",
    "\n",
    "    def state(self, observation):\n",
    "        pos = torch.tensor(\n",
    "            observation[\"env\"][\"position\"], dtype=torch.float, device=self.device\n",
    "        )\n",
    "        pos = self.penc(pos).flatten()\n",
    "        return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f252a63c-a4b0-4499-853a-861551796371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class TrajectoryDoneWrapper(gym.Wrapper):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        max_mistake_duration,\n",
    "        respawn_sequence=[\"launched\", \"standing\"] * 2,\n",
    "        give_up_before_first_cp=True,\n",
    "    ):\n",
    "        super().__init__(env)\n",
    "        self.max_mistake_duration = max_mistake_duration\n",
    "        self.respawn_sequence = respawn_sequence\n",
    "        self.give_up_before_first_cp = give_up_before_first_cp\n",
    "        self.max_respawn = len(respawn_sequence)\n",
    "        assert hasattr(self, \"trajectory\"), \"Env must have TrajectoryEnvWrapper\"\n",
    "\n",
    "    def update_observation(self, observation):\n",
    "        observation[\"env\"][\"trajectory\"][\n",
    "            \"max_mistake_duration\"\n",
    "        ] = self.max_mistake_duration\n",
    "        if (not self.give_up_before_first_cp or self.checkpoints) and not observation[\n",
    "            \"env\"\n",
    "        ][\"finished\"]:\n",
    "            observation[\"env\"][\"trajectory\"][\"max_respawn\"] = self.max_respawn\n",
    "        else:\n",
    "            observation[\"env\"][\"trajectory\"][\"max_respawn\"] = 0\n",
    "        observation[\"env\"][\"trajectory\"][\"respawn_count\"] = self.respawn_count\n",
    "        observation[\"env\"][\"trajectory\"][\"respawn_progress\"] = self.respawn_progress\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, done, info = self.env.step(action)\n",
    "        traj_obs = observation[\"env\"][\"trajectory\"]\n",
    "        exceed_mistake_duration = (\n",
    "            self.max_mistake_duration < traj_obs[\"mistake_duration\"]\n",
    "        )\n",
    "        if exceed_mistake_duration and not done:\n",
    "            if (\n",
    "                not self.give_up_before_first_cp or self.checkpoints\n",
    "            ) and self.respawn_count < self.max_respawn:\n",
    "                info[\"must_respawn\"] = True\n",
    "            else:\n",
    "                done = True\n",
    "\n",
    "        if observation[\"env\"][\"crossed_checkpoint\"] or observation[\"env\"][\"finished\"]:\n",
    "            self.respawn_count = 0\n",
    "            self.respawn_progress = observation[\"env\"][\"trajectory\"][\"progress\"]\n",
    "\n",
    "        self.update_observation(observation)\n",
    "        info[\"total_respawn_count\"] = self.total_respawn_count\n",
    "        info[\"respawn_type\"] = self.respawn_sequence[\n",
    "            self.respawn_count % self.max_respawn\n",
    "        ]\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def respawn(self, launched=True):\n",
    "        observation = self.env.respawn(launched)\n",
    "        self.respawn_count += 1\n",
    "        self.total_respawn_count += 1\n",
    "        self.respawn_progress = observation[\"env\"][\"trajectory\"][\"progress\"]\n",
    "        self.update_observation(observation)\n",
    "        return observation\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        observation = self.env.reset(**kwargs)\n",
    "        self.respawn_count = 0\n",
    "        self.total_respawn_count = 0\n",
    "        self.respawn_progress = 0\n",
    "        self.update_observation(observation)\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbf9e8e-b1c4-48bd-ac09-4948ff73a7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class TrajectoryTravelRewardWrapper(gym.Wrapper):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        mistake_reward,\n",
    "        discount_factor,\n",
    "        penalty_threshold=0.0,\n",
    "    ):\n",
    "        super().__init__(env)\n",
    "        self.mistake_reward = mistake_reward\n",
    "        self.discount_factor = discount_factor\n",
    "        self.penalty_threshold = penalty_threshold\n",
    "        self.div = 100/3.6 # Normalize the reward, the AI will spend a lot of time around 100kmh\n",
    "        self.min_reward = self.mistake_reward / (1 -  self.discount_factor) / self.div\n",
    "        self.max_reward = (1000/3.6) / (1 -  self.discount_factor) / self.div\n",
    "        assert hasattr(self, \"trajectory\"), \"Env must have TrajectoryEnvWrapper\"\n",
    "\n",
    "    def distance_penalty(self, distance):\n",
    "        if distance < self.penalty_threshold:\n",
    "            return 1.0\n",
    "        elif self.max_distance < distance:\n",
    "            return 0.0\n",
    "        return 1.0 - (distance - self.penalty_threshold) / (\n",
    "            self.max_distance - self.penalty_threshold\n",
    "        )\n",
    "\n",
    "    def travel_reward(self, env_obs):\n",
    "        reward = env_obs[\"trajectory\"][\"step_travel\"]\n",
    "        reward *= self.distance_penalty(env_obs[\"trajectory\"][\"weighted_distance\"])\n",
    "        reward /= env_obs[\"_updatetime\"] - self.previous_updatetime\n",
    "        return reward\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, done, info = self.env.step(action)\n",
    "        if observation[\"env\"][\"finished\"]:\n",
    "            reward += observation[\"env\"][\"velocity_norm\"] / (1 -  self.discount_factor)\n",
    "        elif done or info[\"must_respawn\"]:\n",
    "            reward += self.mistake_reward / (1 -  self.discount_factor)\n",
    "        elif observation[\"env\"][\"trajectory\"][\"mistake\"]:\n",
    "            reward += self.mistake_reward\n",
    "        else:\n",
    "            reward += self.travel_reward(observation[\"env\"])\n",
    "\n",
    "        reward /= self.div\n",
    "\n",
    "        self.previous_updatetime = observation[\"env\"][\"_updatetime\"]\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def respawn(self, launched=True):\n",
    "        observation = self.env.respawn(launched)\n",
    "        self.previous_updatetime = observation[\"env\"][\"_updatetime\"]\n",
    "        return observation\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        observation = self.env.reset(**kwargs)\n",
    "        self.previous_updatetime = observation[\"env\"][\"_updatetime\"]\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae67db87-0c99-4c8a-80d1-9c20f8b3d60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class EpisodeInfoWrapper(gym.Wrapper):\n",
    "    def reset(self, **kwargs):\n",
    "        observation = self.env.reset(**kwargs)\n",
    "        self.segments = []\n",
    "        self.segment_reward = 0\n",
    "        self.segment_step_count = 0\n",
    "        self.total_reward = 0\n",
    "        self.total_step_count = 0\n",
    "        self.segment_racetime = 0\n",
    "        self.no_respawn_racetime_offset = 0\n",
    "        self.no_respawn_reward_offset = 0\n",
    "        self.no_respawn_step_count_offset = 0\n",
    "        self.medal = \"unfinished\"\n",
    "        return observation\n",
    "\n",
    "    def add_segment(self, type, racetime, duration):\n",
    "        self.segments.append(\n",
    "            dict(\n",
    "                type=type,\n",
    "                racetime=racetime,\n",
    "                duration=duration,\n",
    "                reward=self.segment_reward,\n",
    "                step_count=self.segment_step_count,\n",
    "            )\n",
    "        )\n",
    "        self.segment_reward = 0\n",
    "        self.segment_step_count = 0\n",
    "        self.segment_racetime = racetime\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, done, info = self.env.step(action)\n",
    "        self.segment_reward += reward\n",
    "        self.segment_step_count += 1\n",
    "        self.total_reward += reward\n",
    "        self.total_step_count += 1\n",
    "        if observation[\"env\"][\"crossed_checkpoint\"]:\n",
    "            duration = info[\"racetime\"] - self.segment_racetime\n",
    "            self.add_segment(\"checkpoint\", info[\"racetime\"], duration)\n",
    "        if observation[\"env\"][\"finished\"]:\n",
    "            info[\"finish_racetime\"] = info[\"racetime\"]\n",
    "            info[\"no_respawn_finish_racetime\"] = (\n",
    "                info[\"racetime\"] - self.no_respawn_racetime_offset\n",
    "            )\n",
    "            if info[\"racetime\"] <= self.map_data[\"medals\"][\"author\"]:\n",
    "                self.medal = \"author\"\n",
    "            elif info[\"racetime\"] <= self.map_data[\"medals\"][\"gold\"]:\n",
    "                self.medal = \"gold\"\n",
    "            elif info[\"racetime\"] <= self.map_data[\"medals\"][\"silver\"]:\n",
    "                self.medal = \"silver\"\n",
    "            elif info[\"racetime\"] <= self.map_data[\"medals\"][\"bronze\"]:\n",
    "                self.medal = \"bronze\"\n",
    "            else:\n",
    "                self.medal = \"no_medal\"\n",
    "        info |= dict(\n",
    "            segments=self.segments,\n",
    "            total_reward=self.total_reward,\n",
    "            total_step_count=self.total_step_count,\n",
    "            no_respawn_racetime=info[\"racetime\"] - self.no_respawn_racetime_offset,\n",
    "            no_respawn_reward=self.total_reward - self.no_respawn_reward_offset,\n",
    "            no_respawn_step_count=self.total_step_count\n",
    "            - self.no_respawn_step_count_offset,\n",
    "            medal=self.medal,\n",
    "        )\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def respawn(self, launched=True):\n",
    "        observation = self.env.respawn(launched)\n",
    "        duration = observation[\"env\"][\"racetime\"] - self.segment_racetime\n",
    "        self.no_respawn_racetime_offset += duration\n",
    "        self.no_respawn_reward_offset += self.segment_reward\n",
    "        self.no_respawn_step_count_offset += self.segment_step_count\n",
    "        self.add_segment(\"respawn\", observation[\"env\"][\"racetime\"], duration)\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e810c04c-5e3e-4c88-97d4-12859377a056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "# |eval: false\n",
    "from nbdev.doclinks import nbdev_export\n",
    "\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73009994",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
